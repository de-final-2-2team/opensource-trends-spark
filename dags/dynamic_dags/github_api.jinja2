from airflow import DAG
from airflow.models import Variable
from airflow.decorators import task, task_group
from airflow.exceptions import (
    AirflowException,
    AirflowBadRequest,
    AirflowNotFoundException,
    AirflowFailException
)
from airflow.utils.trigger_rule import TriggerRule
from datetime import datetime
from datetime import timedelta

from plugins.common import deep_get
from plugins.file_ops import load_as_json
from plugins.github_api import get_request

import requests
import logging
import json
import os


@task(trigger_rule=TriggerRule.ONE_FAILED, retries=1)
def extract(kind, url, params=None):
    return get_request(kind=kind, url=url, params=params)


@task
def transform(kind, columns, response):
    logging.info(f"[{{ dag_id }}:{kind}] 데이터 변환 시작")
    return [{new_col: deep_get(item, old_col) for new_col, old_col in columns.items()} for item in response.get('items', [])]


@task
def load(kind, content):
    dag_root_path = os.path.dirname(os.path.abspath(__file__))
    load_as_json(dag_root_path, kind, content)


@task_group
def get_data():
    tasks = {{ tasks }}
    result = {}
    for kind, task in tasks.items():
        try:
            if 'columns' in task:
                result[kind] = transform(kind=kind, columns=task.get('columns'), response=extract(kind=kind, url=task.get('url'), params=task.get('params')))
            else:
                result[kind] = extract(kind=kind, url=task.get('url'), params=task.get('params'))
        except Exception as e:
            logging.error(f"[{{ dag_id }}:{kind}] 데이터 수집 실패\\n" + repr(e))
            result[kind] = None
    return result


@task
def remove_duplicates(content):
    unique_items = []
    item_keys = set()

    for value in content.values():
        for item in value:
            item_id = item["NODE_ID"]
            if item_id not in item_keys:
                unique_items.append(item)
                item_keys.add(item_id)
    return unique_items

with DAG(
    dag_id="github_{{ dag_id }}",
    start_date=datetime(2023, 6, 15),
    schedule='{{ schedule }}',
    catchup={{ catchup or True }}
) as dag:
    contents = get_data()
    load(kind="{{ dag_id }}", content=get_data())

    if '{{ dag_id }}' == 'repo':
        unique_repos = remove_duplicates(contents)